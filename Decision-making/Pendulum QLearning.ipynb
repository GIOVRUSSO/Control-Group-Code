{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(Z, Zdim, Zmin, Zstep):\n",
    "    ###\n",
    "    #Discretize a variable Z using its dimension Zdim, its minimal values along each axis and the discretization steps\n",
    "    res = [0]*Zdim #n-dimensional index\n",
    "    for i in range(Zdim): #For each dimension\n",
    "        elt = Z[i] #Extract the i-th element\n",
    "        ind = int((elt - Zmin[i])//Zstep[i]) #Discretize\n",
    "        res[i] = ind\n",
    "    return(tuple(res)) #Return as tuple for array indexing\n",
    "\n",
    "def DKL(f,g):\n",
    "    div = 0\n",
    "    for i in range(51):\n",
    "        for j in range(51):\n",
    "            if g[i][j] != 0 and f[i][j] != 0:\n",
    "                div = div + f[i][j]\n",
    "    return(div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_axis = [-2.5 + (n+0.5)*0.25 for n in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pendulum():\n",
    "    def __init__(self):\n",
    "        self.l = 0.6\n",
    "        self.m = 1\n",
    "        self.max_torque = 10\n",
    "        self.max_speed = 5\n",
    "        self.state = np.array([0,0])\n",
    "        self.last_input = None\n",
    "        self.g = 9.81\n",
    "        self.dt = 0.1\n",
    "        self.angle_std = 3*2*np.pi/360\n",
    "        self.speed_std = 0.1\n",
    "        \n",
    "    def random_init(self):\n",
    "        ###\n",
    "        #Random initialization\n",
    "        self.state = np.array([np.random.uniform(-np.pi, np.pi),np.random.uniform(-sefl.max_speed, self.max_speed)])\n",
    "        \n",
    "    def step(self, u):\n",
    "        #Simulation step\n",
    "        u = np.clip(u, -self.max_torque, self.max_torque) #Clip the input as safety\n",
    "        self.last_input = u\n",
    "        accel = u/(self.m*self.l*self.l) + self.g*np.sin(self.state[0])/self.l #Dynamics for the acceleration\n",
    "        \n",
    "        speed = self.state[1]\n",
    "        angle = self.state[0]\n",
    "        \n",
    "        new_speed = speed + accel*self.dt + np.random.normal(0,self.speed_std) #Calculate the new speed\n",
    "        new_speed = np.clip(new_speed, -self.max_speed, self.max_speed) #Clip the speed as safety (in practice the bounds were chosen to ensure this very rarely happens)\n",
    "        \n",
    "        new_angle = angle + speed*self.dt + np.random.normal(0,self.angle_std) #New angle\n",
    "        if new_angle < -np.pi: #Angle is modulo 2\\pi\n",
    "            new_angle = new_angle + 2*np.pi\n",
    "        if new_angle > np.pi:\n",
    "            new_angle = new_angle - 2*np.pi\n",
    "        \n",
    "        self.state = np.array([new_angle, new_speed])\n",
    "        \n",
    "    def set_state(self, angle, speed):\n",
    "        ###\n",
    "        #Explicitely set the state\n",
    "        speed = np.clip(speed, -self.max_speed, self.max_speed)\n",
    "        if angle < -np.pi:\n",
    "            angle = angle + 2*np.pi\n",
    "        if angle > np.pi:\n",
    "            angle = angle - 2*np.pi\n",
    "        self.state = np.array([angle, speed])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning():\n",
    "    def __init__(self):\n",
    "        ###\n",
    "        #Initialize the Q-table and the parameters\n",
    "        self.values = np.zeros((50,50,20))\n",
    "        self.learning_rate = 0.5 #High learning rate for fast learning\n",
    "        self.discount = 0.99\n",
    "        self.epsilon = 0.9\n",
    "        self.P = Pendulum()\n",
    "        self.P.set_state(np.pi, 0)\n",
    "        \n",
    "    def reward(self, state):\n",
    "        ###\n",
    "        #Reward of a state\n",
    "        return(-state[0]*state[0] - state[1]*state[1]*0.1)\n",
    "    \n",
    "    def Qval_update(self, r, s, snew, a):\n",
    "        ###\n",
    "        #Classical Q-value update given a reward, two states and an action\n",
    "        stateInd = discretize(s, 2, [-np.pi, -5], [2*np.pi/50, 0.2]) #Discretize all values\n",
    "        aInd = discretize([a], 1, [-2.5], [0.25])\n",
    "        newStateInd = discretize(snew, 2, [-np.pi, -5], [2*np.pi/50, 0.2])\n",
    "        self.values[stateInd[0],stateInd[1],aInd[0]] = (1-self.learning_rate)*self.values[stateInd[0],stateInd[1],aInd[0]] + self.learning_rate*(r + self.discount*max(self.values[newStateInd])) #Q-value update rule\n",
    "    \n",
    "    def training_step(self):\n",
    "        ###\n",
    "        #Training step: pick an action (\\epsilon-greedy policy), update Q-val\n",
    "        expl = np.random.rand() #Greedy or random selection\n",
    "        state = self.P.state\n",
    "        if expl>self.epsilon:\n",
    "            u = np.random.choice(u_axis)\n",
    "        else:\n",
    "            stateInd = discretize(self.P.state, 2, [-np.pi, -5], [2*np.pi/50, 0.2])\n",
    "            uind = np.argmax(self.values[stateInd]) #Pick the greedy action for the current state\n",
    "            u = u_axis[uind]\n",
    "        self.P.step(u) #Simulation step\n",
    "        newState = self.P.state #Record the new state\n",
    "        rwd = self.reward(state) #Get reward\n",
    "        self.Qval_update(rwd, state, newState, u) #Q-val update\n",
    "        return(rwd)\n",
    "    \n",
    "    def exploitation_step(self):\n",
    "        ###\n",
    "        #Similar to a training step but uses the greedy policy\n",
    "        state = self.P.state\n",
    "        stateInd = discretize(self.P.state, 2, [-np.pi, -5], [2*np.pi/50, 0.2])\n",
    "        uind = np.argmax(self.values[stateInd])\n",
    "        u = u_axis[uind]\n",
    "        self.P.step(u)\n",
    "        newState = self.P.state\n",
    "        rwd = self.reward(state)\n",
    "        self.Qval_update(rwd, state, newState, u)\n",
    "        return(u)\n",
    "    \n",
    "    def episode(self):\n",
    "        ###\n",
    "        #Training episode, 500 steps\n",
    "        self.P.set_state(np.pi, 0)\n",
    "        tot = 0\n",
    "        for i in range(500):\n",
    "            r = self.training_step()\n",
    "        tot = tot + r\n",
    "        return(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = QLearning() #Initialize a Q-learning, we will train it and save it along checkpoints\n",
    "for i in range(20): #10k steps\n",
    "    Q.episode()\n",
    "Q20 = QLearning() #First checkpoint\n",
    "Q20.values = Q.values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(180): #100k steps\n",
    "    Q.episode()\n",
    "Q200 = QLearning()\n",
    "Q200.values = Q.values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1800): #1M steps\n",
    "    Q.episode()\n",
    "Q2000 = QLearning()\n",
    "Q2000.values = Q.values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(18000): #10M steps\n",
    "    Q.episode()\n",
    "Q20k = QLearning()\n",
    "Q20k.values = Q.values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(80000): #50M steps\n",
    "    Q.episode()\n",
    "Q100k = QLearning()\n",
    "Q100k.values = Q.values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeans(QL):\n",
    "    ###\n",
    "    #Perform 50 simulations and averagle the resulting rewards for performance analysis\n",
    "    fullH = np.zeros((50,299))\n",
    "    uH = np.zeros((50,299))\n",
    "    nSteps = 300\n",
    "\n",
    "    for j in range(50): #Perform 50 simulations\n",
    "        hist = [[0,0]]*nSteps #Initialize empty reward and input histories\n",
    "        uhist = [0]*nSteps\n",
    "        QL.P.set_state(np.pi, 0) #(re)nitialize the pendulum\n",
    "        for i in range(nSteps-1):\n",
    "            state = QL.P.state #Get state\n",
    "            u = QL.exploitation_step() #Exploitation step\n",
    "            hist[i+1] = -state[0]*state[0] - state[1]*state[1]*0.1 #Log reward\n",
    "            uhist[i] = u\n",
    "        fullH[j] = hist[1:]\n",
    "        uH[j] = uhist[:299] #We won't return the input history here, but the function can be modified to do so\n",
    "    means = [0]*299\n",
    "    stds = [0]*299\n",
    "    for i in range(299):\n",
    "        means[i] = np.mean(fullH[:,i])\n",
    "        stds[i] = np.std(fullH[:,i])\n",
    "    return(means, stds)\n",
    "\n",
    "\n",
    "\n",
    "def getMeans2(QL):\n",
    "    ###\n",
    "    #Perform 50 simulations and average the angle and input for experiments\n",
    "    fullH = np.zeros((50,299))\n",
    "    uH = np.zeros((50,299))\n",
    "    nSteps = 300\n",
    "\n",
    "    for j in range(50): #Do 50 simulations\n",
    "        hist = [[0,0]]*nSteps #Initialize state and input histories\n",
    "        uhist = [0]*nSteps\n",
    "        QL.P.set_state(np.pi, 0) #(re)nitialize the pendulum\n",
    "        for i in range(nSteps-1):\n",
    "            state = QL.P.state #Get state\n",
    "            u = QL.exploitation_step() #Step\n",
    "            hist[i+1] = state #Log state\n",
    "            uhist[i] = u #Log input\n",
    "        fullH[j] = [x[0] for x in hist[1:]]\n",
    "        uH[j] = uhist[:299]\n",
    "    means = [0]*299\n",
    "    stds = [0]*299\n",
    "    umeans = [0]*299\n",
    "    ustds = [0]*299\n",
    "    for i in range(299): #Mean and std for state and input for every point\n",
    "        means[i] = np.mean(fullH[:,i])\n",
    "        stds[i] = np.std(fullH[:,i])\n",
    "        umeans[i] = np.mean(uH[:,i])\n",
    "        ustds[i]= np.std(uH[:,i])\n",
    "    return(means, stds, umeans, ustds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#This cell generates the data for the 'average reward' bar plot\n",
    "\n",
    "res = [0]*6\n",
    "res2 = [0]*6\n",
    "QLs = [Q20, Q200, Q2000, Q20k, Q100k, Q500k]\n",
    "for i in range(6):\n",
    "    means, stds = getMeans(QLs[i])\n",
    "    res[i] = np.mean(means[200:])\n",
    "    res2[i] = np.mean(stds[200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(5), res[:5], yerr=res2[:5], align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "ax.set_ylabel('Mean reward')\n",
    "ax.set_xticklabels(['0', '20', '200', '2k', '20k', '100k'])\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "plt.savefig('bar_plot_with_error_bars.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#Use our predefined function to get the average trajectories for two checkpoints\n",
    "\n",
    "Q2000bis = QLearning()\n",
    "Q2000bis.values = Q2000.values.copy()\n",
    "\n",
    "means, stds, umeans, ustds = getMeans2(Q2000bis)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pylab import rcParams\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "x = np.array([x/10 for x in range(299)])\n",
    "y = np.array(means)\n",
    "ci = np.array(stds)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x,y)\n",
    "ax.fill_between(x, (y-ci), (y+ci), color='b', alpha=.1)\n",
    "#ax.set_title('FPD input')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Angular position')\n",
    "plt.savefig('QL_angle_ci.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([x/10 for x in range(299)])\n",
    "y = np.array(umeans)\n",
    "ci = np.array(ustds)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x,y)\n",
    "ax.fill_between(x, (y-ci), (y+ci), color='b', alpha=.1)\n",
    "#ax.set_title('FPD input')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Control input')\n",
    "plt.savefig('QL_input_ci.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q100kbis = QLearning()\n",
    "Q100kbis.values = Q100k.values.copy()\n",
    "\n",
    "means, stds, umeans, ustds = getMeans2(Q100kbis)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pylab import rcParams\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "x = np.array([x/10 for x in range(299)])\n",
    "y = np.array(means)\n",
    "ci = np.array(stds)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x,y)\n",
    "ax.fill_between(x, (y-ci), (y+ci), color='b', alpha=.1)\n",
    "#ax.set_title('FPD input')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Angular position')\n",
    "plt.savefig('QL_angle_ci_100k.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([x/10 for x in range(299)])\n",
    "y = np.array(umeans)\n",
    "ci = np.array(ustds)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x,y)\n",
    "ax.fill_between(x, (y-ci), (y+ci), color='b', alpha=.1)\n",
    "#ax.set_title('FPD input')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Control input')\n",
    "plt.savefig('QL_input_ci_100k.png', bbox_inches = 'tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
