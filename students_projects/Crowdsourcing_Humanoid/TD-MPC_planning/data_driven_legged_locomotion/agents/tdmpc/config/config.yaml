defaults:
    - override hydra/launcher: submitit_local

# environment
task: h1-walk-v1 #humanoid_h1-walk-v0
obs: state

# evaluation
checkpoint: None
experiment_name: None

eval_episodes: 1
eval_freq: 150000

from_scratch: false

# high level parameters (my-rice)
max_episode_steps: 5000 # default: 1000
frame_skip: 1 # default: 10

generalize_movement: true

# training
steps: 5_000_000
batch_size: 256
reward_coef: 0.1
value_coef: 0.1
consistency_coef: 20
rho: 0.5
lr: 3e-4
enc_lr_scale: 0.3
grad_clip_norm: 20
tau: 0.01
discount_denom: 5
discount_min: 0.95
discount_max: 0.995
buffer_size: 1_000_000 # default: 1_000_000
exp_name: tdmpc
data_dir: ??? # This parameter is for the data directory where the data is stored. It is used for the offline RL.
save_freq: 30_000

# planning
mpc: true
iterations: 6
num_samples: 512
num_elites: 64
num_pi_trajs: 24
horizon: 3 # default: 3. TODO(my-rice): Try to increase this parameter to 5 or 7
min_std: 0.05
max_std: 2
temperature: 0.5

# actor
log_std_min: -10
log_std_max: 2
entropy_coef: 1e-4

# critic
num_bins: 101
vmin: -10
vmax: +10

# architecture
model_size: ???
num_enc_layers: 2
enc_dim: 256
num_channels: 32
mlp_dim: 512
latent_dim: 512
task_dim: 0 # I did not understand this parameter. This parameter will be set to 0 by the code. So I don't know what it is for.
num_q: 5
dropout: 0.01
simnorm_dim: 8

# logging
wandb_project: ???
wandb_entity: ??? 
wandb_silent: false
disable_wandb: true
save_csv: true

# misc
save_video: true
save_agent: true
seed: 1

# convenience
work_dir: ???
task_title: H1 Walk V0
multitask: false
tasks: ['h1-walk-v0']
obs_shape: {'state': [51]}
action_dim: 19
episode_length: 5000
obs_shapes: ???
action_dims: ???
episode_lengths: ???
seed_steps: 25000
bin_size: 0.2
